# @package _global_

# maestro_upsample_lvl2

defaults:
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /datamodule: maestro_datamodule.yaml
  - override /model: jukebox_upsampler

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["run"]

hydra:
  run:
    dir: ${paths.log_dir}/${task_name}/runs/up_lvl2

ckpt_path: ${paths.output_dir}/checkpoints/last.ckpt

seed: 501

trainer:
  max_steps: 500_000

datamodule:
  root_dir: ${oc.env:MAESTRO_DATASET_DIR}
  num_workers: 4
  batch_size: 20
  sample_length: 524288

model:
  source_lvl: 2
  target_lvl: 1
  model:
    _target_: src.module.sgconv.GConvStackedDiffusion
    input_dim: 64
    cond_dim: 64
    time_emb_dim: 32
    model_dim: 64
    channels: 1
    depth: 8
    timestep_max_index: 1000
    l_max: 16_384
  lr: 5e-5
  # no scheduler -> constant lr
  # lr_scheduler:
  #   _target_: cosine_annealing_warmup.CosineAnnealingWarmupRestarts
  #   first_cycle_steps: 250_000
  #   cycle_mult: 1.0
  #   max_lr: 1e-4 # this overwrites the lr parameter
  #   min_lr: 1e-5
  #   warmup_steps: 10_000
  #   gamma: 0.5
  num_train_timesteps: 1000
  inference_batch_size: 8
  noise_scheduler:
    _target_: diffusers.DPMSolverMultistepScheduler
    num_train_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: linear
    solver_order: 3
    prediction_type: sample
  num_inference_steps: 50
  source_noise_std: 1.0
  source_dropout: 0.0
  guidance_scales: [1]
  source_normalizer_path: "normalizations/maestro_lvl_2.pt"
  target_normalizer_path: "normalizations/maestro_lvl_1.pt"
  timestep_sampler:
    _target_: src.diffusion.timestep_sampler.TimeConstantSampler
    max_timestep: 1000
  skip_audio_logging: false
  log_train_audio: true
  clip_embeddings: true


logger:
  wandb:
    project: "jukebox_diffusion"
    tags: ${tags}
    name: "up_lvl2"
    id: "up_lvl2"
  tensorboard:
    name: "jukebox_diffusion"
